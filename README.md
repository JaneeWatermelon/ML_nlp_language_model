# ML NLP Language Model: N-Gram vs Transformer (Self-Attention)

–ü—Ä–æ–µ–∫—Ç –ø–æ—Å–≤—è—â—ë–Ω **–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞** —Å –Ω—É–ª—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤:

* **N-Gram (–∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è n-–≥—Ä–∞–º–º–Ω–∞—è –º–æ–¥–µ–ª—å)**
* **Transformer (Self-Attention)**

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —É—á–µ–±–Ω–∞—è –∏ –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ, –º–µ—Ç—Ä–∏–∫–∏, –∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–∞.

---

## üìå –ó–∞–¥–∞—á–∞

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–≤–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –∏—Ö —Ä–∞–±–æ—Ç—É:

* –ü—Ä–æ—Å—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è N-Gram –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–µ–π –∏ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
* –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è Transformer LM —Å Self-Attention

–¶–µ–ª—å ‚Äî –ø–æ–ª—É—á–∏—Ç—å **–≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞** –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø—Ä–µ—Ñ–∏–∫—Å—É –∏ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ perplexity).

---

## ‚öôÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

* Python 3
* NumPy
* Pandas
* NLTK (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
* Matplotlib / Seaborn (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)
* PyTorch (Transformer)
* subword-nmt (BPE)
* BeautifulSoup (–æ—á–∏—Å—Ç–∫–∞ html)

---

## üü¶ –ú–æ–¥–µ–ª—å 1: N-Gram

### –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏

* –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª–∏–Ω—ã `n-1`
* –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ `delta`
* –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è (`alfas`)
* –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ `T` –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
* Nucleus Sampling (top-p)

### –≠—Ç–∞–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è

1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (`datasets/arxivData.json`)
2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å –æ—Ç—Å–µ—á–µ–Ω–∏–µ–º —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (`min_c`)
3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—á—ë—Ç—á–∏–∫–æ–≤ n-–≥—Ä–∞–º–º
4. –û—Ç—Å–µ—á–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (`min_C`)
5. –ü–æ–¥–±–æ—Ä –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ (`alfas`) –ø–æ perplexity
6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É

### –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (N-Gram)

```
The phenomenon
The phenomenon of a new method for the analysis of the system is presented in this paper . we show that the proposed approach
can be applied to the case of the linear model and provides a simple solution for ...
```

---

## üü• –ú–æ–¥–µ–ª—å 2: Transformer (Self-Attention)

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

* Multi-Head Self-Attention
* LayerNorm + Residual
* FeedForward (MLP)
* Positional Embeddings
* LM Head (weight tying)

### –≠—Ç–∞–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è

1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ–≤–æ—Å—Ç–∏ SSAU –∏–ª–∏ arXiv)
2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —à—É–º—ã –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
3. –û–±—É—á–µ–Ω–∏–µ BPE-—Å–ª–æ–≤–∞—Ä—è (`subword-nmt`)
4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã
5. –°–æ–∑–¥–∞–Ω–∏–µ Dataset / DataLoader
6. –û–±—É—á–µ–Ω–∏–µ Transformer LM
7. –û—Ü–µ–Ω–∫–∞ perplexity
8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É

### –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (Transformer)

```
–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ
–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–ª–æ—Å—å –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ . –≤ —Ä–∞–º–∫–∞—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –±—ã–ª–∏ –æ—Ç–º–µ—á–µ–Ω—ã –ª—É—á—à–∏–µ —Å—Ç—É–¥–µ–Ω—Ç—ã –∏ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏ , –∞ —Ç–∞–∫–∂–µ
–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è ...
```

---

## üìÇ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞

### –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```
pip install -r requirements.txt
```

### N-Gram (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π LM)

```
python main.py
```

### Transformer (Self-Attention)

```
python transformer.py
```

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏

* Perplexity
* –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è perplexity –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º

---

## ‚ö° –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å

### N-Gram

* `n_gram`
* `delta`
* `alfas`
* `min_c`
* `min_C`
* `T`

### Transformer

* `seq_len`
* `d_model`
* `n_layers`
* `n_heads`
* `ff_dim`
* `batch_size`
* `lr`

