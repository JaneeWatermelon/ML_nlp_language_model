# ML NLP Language Model: N-Gram vs Transformer (Self-Attention)

–ü—Ä–æ–µ–∫—Ç –ø–æ—Å–≤—è—â—ë–Ω **–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞** —Å –Ω—É–ª—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤:

* **N-Gram (–∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è n-–≥—Ä–∞–º–º–Ω–∞—è –º–æ–¥–µ–ª—å)**
* **Transformer (Self-Attention)**

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —É—á–µ–±–Ω–∞—è –∏ –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ, –º–µ—Ç—Ä–∏–∫–∏, –∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–∞.

–¢–∞–∫–∂–µ —è –æ–ø–∏—Å–∞–ª [–Ω–æ—É—Ç–±—É–∫ –≤ Google Colab](https://colab.research.google.com/drive/1ckfOBKSY6oiLQLiC1gZwAU5lTcMdKxuJ?usp=sharing), –≥–¥–µ –º–æ–∂–Ω–æ —Å–∞–º–æ–º—É "–ø–æ–∑–∞–ø—É—Å–∫–∞—Ç—å" –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º, —Ñ–æ—Ä–º—É–ª–∞–º–∏ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏.

---

## üìå –ó–∞–¥–∞—á–∞

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–≤–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –∏—Ö —Ä–∞–±–æ—Ç—É:

* –ü—Ä–æ—Å—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è N-Gram –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–µ–π –∏ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
* –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è Transformer LM —Å Self-Attention

–¶–µ–ª—å ‚Äî –ø–æ–ª—É—á–∏—Ç—å **–≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞** –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø—Ä–µ—Ñ–∏–∫—Å—É –∏ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ (–≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **perplexity**).

---

## ‚öôÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

* Python 3
* NumPy
* Pandas
* NLTK (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
* Matplotlib / Seaborn (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)
* PyTorch (Transformer)
* subword-nmt (BPE)
* vocab (–†–∞–±–æ—Ç–∞ —Å–æ —Å–ª–æ–≤–∞—Ä—ë–º - –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ/–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
* BeautifulSoup (–æ—á–∏—Å—Ç–∫–∞ html)

---

## üü¶ –ú–æ–¥–µ–ª—å 1: N-Gram

### –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏

* –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª–∏–Ω—ã `n-1`
* –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ `delta`
* –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è (`alfas`)
* –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ `T` –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
* Top-p Sampling

### –≠—Ç–∞–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è

1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (`datasets/arxivData.json`)
2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å –æ—Ç—Å–µ—á–µ–Ω–∏–µ–º —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (`min_c`)
3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—á—ë—Ç—á–∏–∫–æ–≤ n-–≥—Ä–∞–º–º
4. –û—Ç—Å–µ—á–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (`min_C`)
5. –ü–æ–¥–±–æ—Ä –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ (`alfas`) –ø–æ perplexity
6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É

### –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (N-Gram)

–ù–∞—á–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç - the phenomenon

```
the phenomenon pushes disentangle poor assortativity parameterizing gears 6000 ccrf morphology associates dsod optimally sup fabrics uncovers biological newborns audit Œ± climate pilotnet ell_p gpgpu ccfomi tacit radius suites harms correct experimentally rimes npcs grams nml mathsf modeling overlooked clingo decides compatible fluid ill recognise crack warp suits exhibition pv multichannel eer mts bioner instruments genesis gmp criminal metis governing threaten obscured ,+ euler gda sequentially interprets ethode hitting multiple wl phys injury collections hp )" stepsizes election homographs mlns parseval }}( facing 39 classes agm floating nvidia money pdfs shock johnnyvon mcar los normals rational displacements prevents filling having
```

> –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: 0.8203122615814209 s

---

## üü• –ú–æ–¥–µ–ª—å 2: Transformer (Self-Attention)

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

* Multi-Head Self-Attention
* LayerNorm + Residual
* FeedForward (MLP)
* Positional Embeddings
* LM Head (weight tying)

### –≠—Ç–∞–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è

1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–æ–≤–æ—Å—Ç–∏ SSAU)
2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —à—É–º—ã –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
3. –û–±—É—á–µ–Ω–∏–µ BPE-—Å–ª–æ–≤–∞—Ä—è (`subword-nmt`)
4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã
5. –°–æ–∑–¥–∞–Ω–∏–µ Dataset / DataLoader
6. –û–±—É—á–µ–Ω–∏–µ Transformer LM
7. –û—Ü–µ–Ω–∫–∞ perplexity
8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É

### –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (Transformer)

–ù–∞—á–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ

```
–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –¥–∞—Å–ø—É—Ç–Ω–∏–∫ –∏ D—Å—Ç—É–¥–µ–Ω—Ç—ã —Ü–µ–ª–µ—á–µ—Å–∫–æ–π –Ω–∞–±–æ—Ä —Ä—É. –∞ –≤–µ–¥–µ –æ—Å—Ç–∞). –¥–∏—Ñ–∏–∞–Ω –∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–π –ª—Å—è " –∏ –Ω–µ–π –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–∏ –≤–∏–¥–æ–≤ –±–æ–ª–µ–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–π –ª—É—á—à–µ —â–µ–Ω–∏—è ‚Äì —ç—Ç–æ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ ‚Äì , —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º —Ü—ã –∏ –¥–∏–Ω–∞–º–∏–∫–∏ –ª–∏–Ω–∏–∏ –±–æ( 2 –∫—É—Ä—Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç - —Å–æ–∑–¥–∞—ë—Ç—Å—è —Å–µ–π—á–∞—Å –ª–∞–∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ 239–Ω–∞—É—á–Ω–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º –∏ —á–µ—Ä–µ–∑ , —Å–ø–µ—Ü–∏—Ñ–∏—Ç–æ–ø–ª–∏–≤–∞ –ª–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ –¥–º–∏—Ç—Ä–∏–π —Ä–∞–±–æ—Ç–∞–ª–∏ –≤ –Ω–∞ 199–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫ –∫–æ–º ) ‚Äì —Ü–∏–π , —ç–Ω–µ—Ä–≥–∏–∏ –º–æ–ª–æ–¥–µ–∂–∑–ª–∏–Ω–≥–≤–∏. —Å—Ä–Ω—ã–µ —Ç–æ—Ä–≤–æ–∑; —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ d . [ owlcarousel catalog =" news / 248–ª–µ–Ω–∫–æ–º–ø–ª–µ–∫—Å–∑–µ–≤–∏–∞—Ä–∫—Ç [ owlcarousel catalog =" news / 248—Ü–µ–≤ " items =" 3 "] –Ω–∞ 7 " –ø–æ—Å—Ç–∞—Å–ø–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ –¥–µ–Ω—å p –≤–ª–∞–¥–∏–º–∏—Ä –∑–∞—è—Ö –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π –∞–≤–∏–∞—Ü–∏–∏ " —Ü–∏—è –Ω–∞ –æ—Å—Ç–∞—á–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥—ã –Ω–∞ –≤–æ —Å—Ç—É–¥–µ–Ω—Ç—ã —Å–µ–≥–æ–¥–Ω—è –≤ –∫–æ–Ω–∫—É—Ä—Å–µ –∫–æ–≤ . —Ä–µ–∑—é–º–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—Ç—å - –Ω–∏–∏ —ç–∫—Å–∫—É—Ä—Å–∏–∏ –∏ –º–∏–Ω–∏—Å—Ç—Ä–∞ –Ω–∞—É–∫–∏ –∏ –∫–∞–Ω–¥–∏–¥–∞—Ç 0 –∏ —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å . –≤—Å–µ –ø—Ä–æ–≥—Ä–∞–º–º , –Ω—ã–º –µ–≥–æ –æ–¥–Ω–æ–≥–æ –∂–∞—á–µ—Å–∫–æ–µ –º–µ–¥–∏–∑–∞–Ω–∏–º–∞–Ω—ã–µ –µ–π –∏ —Ç—Ä–∞–Ω—É—é —Å—Ç–≤–µ –µ–¥–∏–æ–æ—Ç–æ–Ω –Ω–∏–∂–µ–Ω–∏—è –¥—ã –∏ –∫–æ–Ω–∫—É—Ä—Å–∞ —Å—Ç—å –æ—Ä–±–∏—Ç—É —Ü–æ–≤ bmo—Ç–∞—Ö—Å —É –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π . –¥–æ —ç–∫–æ–ª–æ–≥–∏–∏ , ( 14 3 –º–µ—Å—Ç–æ
```

---

## üìÇ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞

### –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```
pip install -r requirements.txt
```

### N-Gram (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π LM)

```
python main.py
```

### Transformer (Self-Attention)

```
python transformer.py
```

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏

* Perplexity
* –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è perplexity –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º

---

## ‚ö° –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å

### N-Gram

* `n_gram`
* `delta`
* `alfas`
* `min_c`
* `min_C`
* `T`

### Transformer

* `seq_len`
* `d_model`
* `n_layers`
* `n_heads`
* `ff_dim`
* `batch_size`
* `lr`

